{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining all the constants required (copying them from Matlab and adjusting syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is process 0 out of 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dolfin import *\n",
    "\n",
    "#pandas is needed to import the cluster database\n",
    "import pandas as pd\n",
    "\n",
    "#Importing MPI for parallel computing\n",
    "# from mpi4py import MPI\n",
    "\n",
    "# #Importing the PETSc module for parallel use\n",
    "# from petsc4py import PETSc\n",
    "\n",
    "#importing mshr for all mesh functions\n",
    "import mshr as mshr\n",
    "\n",
    "# Use SymPy to compute f from the manufactured solution u\n",
    "import sympy as sym\n",
    "\n",
    "#Option to avoid printing redundant information from each core when running the code in parallel from\n",
    "#a python (.py) script obtained from the jupyter notebook.\n",
    "# parallel_run = True\n",
    "\n",
    "#MPI communicator\n",
    "comm = MPI.comm_world\n",
    "\n",
    "#Rank of each process (its ID essentially)\n",
    "rank = MPI.rank(comm)\n",
    "\n",
    "#Total number of processes\n",
    "number_processes = MPI.size(comm)\n",
    "\n",
    "print(f'This is process {rank} out of {number_processes-1}')\n",
    "\n",
    "if number_processes <2:\n",
    "\n",
    "    #Increasing the width of the notebook (visual difference only)\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    \n",
    "    #have to define where to put plots BEFORE importing matplotlib\n",
    "    %matplotlib notebook\n",
    "\n",
    "#Importing matplotlib to plot the results\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Importing numpy to work with arrays\n",
    "import numpy as np\n",
    "\n",
    "#Importing tempfile to save numpy arrays from the main script so we can get them back and plot them\n",
    "#interatively rather than saving a pdf or png!\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#Importing time to compute how long each segment takes\n",
    "import time\n",
    "\n",
    "#importing regex to change every instance of radius_tot so we change the ones in the C++ code\n",
    "#at the same time too\n",
    "import re\n",
    "\n",
    "#varname gives the name of the variable as a string\n",
    "from varname import varname\n",
    "\n",
    "#Needed to use the 3D scatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#Importing the decimal package to be able to specify arbitrary accuracy, needed e.g. when\n",
    "#calculating the jacobian for the lensing\n",
    "from decimal import *\n",
    "\n",
    "#Importing all quantities, constants etc used in the calculations\n",
    "from MONDquantities import *\n",
    "\n",
    "#Importing all classes I created\n",
    "from MONDclasses import *\n",
    "\n",
    "#Importing the functions I made from the MONDfunctions file\n",
    "from MONDfunctions import *\n",
    "\n",
    "#Importing all expressions for weak forms, initial guesses/BCs and sources\n",
    "from MONDexpressions import *\n",
    "\n",
    "#Needed if want to use the adapt function for mesh refinement, see:\n",
    "#https://fenicsproject.org/qa/6719/using-adapt-on-a-meshfunction-looking-for-a-working-example/\n",
    "#If using 'plaza' instead of 'plaza_with_parent_facets', it's faster by about 30%! Also, I get the\n",
    "#'*** Warning: Cannot calculate parent facets if redistributing cells'. So for MPI no need to use\n",
    "#with parent facets!\n",
    "parameters[\"refinement_algorithm\"] = \"plaza\"\n",
    "\n",
    "#Setting compiler parameters.\n",
    "#Optimisation\n",
    "parameters[\"form_compiler\"][\"optimize\"]     = True\n",
    "parameters[\"form_compiler\"][\"cpp_optimize\"] = True\n",
    "\n",
    "#Nonzero initial guess for the Krylov solver. Doesnt seem to make a difference for nonlinear problems\n",
    "parameters['krylov_solver']['nonzero_initial_guess'] = True\n",
    "\n",
    "#Ghost mode for when using MPI. Each process gets ghost vertices for the part of the domain it does not\n",
    "#own. Have to set to 'none' instead or I get Error 'Unable to create BoundaryMesh with ghost cells.'\n",
    "parameters['ghost_mode'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info(parameters,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## starting time of whole PDE solver\n",
    "starting_time = time.time()\n",
    "\n",
    "#starting an empty list to contain all of the run_time objects to plot later\n",
    "section_times = []\n",
    "\n",
    "print('Starting mesh generation...\\n')\n",
    "mesh_generation_start = time.time()\n",
    "\n",
    "#Making mesh from function defined above\n",
    "mesh = make_spherical_mesh(domain_size, mesh_resolution)\n",
    "\n",
    "mesh_generation_end = time.time()\n",
    "mesh_generation_time = run_time(mesh_generation_end - mesh_generation_start, 'Mesh Generation')\n",
    "section_times.append(mesh_generation_time)\n",
    "print('Mesh generated in {} s \\n'.format(mesh_generation_time.time))\n",
    "\n",
    "#Setting the MPI communicator for the mesh (doesnt seem to do anything right now)\n",
    "# mesh.mpi_comm = comm\n",
    "\n",
    "print(f'The mesh of process {rank} has {mesh.num_cells()} cells')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing timing plot for mesh for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plots_for_thesis:\n",
    "\n",
    "    #empty array to store mesh times\n",
    "    mesh_times = []\n",
    "\n",
    "    #range of nesh reoslutions to test\n",
    "    resolutions_to_test = np.arange(10,42,2)\n",
    "\n",
    "    #Number of cells per mesh tested\n",
    "    mesh_test_cells = np.zeros((len(resolutions_to_test), ))\n",
    "\n",
    "    #Number of vertices per mesh tested\n",
    "    mesh_test_vertices = np.zeros((len(resolutions_to_test), ))\n",
    "\n",
    "    #Minimum cell diameter\n",
    "    mesh_test_minimum_diameter = np.zeros((len(resolutions_to_test), ))\n",
    "\n",
    "    for i, resolution in enumerate(resolutions_to_test):\n",
    "\n",
    "        print('Starting mesh generation...\\n')\n",
    "        mesh_generation_start = time.time()\n",
    "\n",
    "        #Making mesh from function defined above\n",
    "        mesh = make_spherical_mesh(domain_size, resolution)\n",
    "\n",
    "        mesh_generation_end = time.time()\n",
    "        mesh_generation_time = run_time(mesh_generation_end - mesh_generation_start, f'{int(resolution)}')\n",
    "        mesh_times.append(mesh_generation_time)\n",
    "\n",
    "        #Storing number of cells, vertices and min diameter in respective arrays. #Cells and #Vertices are\n",
    "        #scaled by 100, minimum diameter is scaled by the domain size\n",
    "        mesh_test_cells[i] = mesh.num_cells()/1000\n",
    "        mesh_test_vertices[i] = mesh.num_vertices()/1000\n",
    "        mesh_test_minimum_diameter[i] = mesh.hmin()/(domain_size*2)\n",
    "\n",
    "        print('Mesh generated in {} s \\n'.format(mesh_generation_time.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    #Storing the quantities needed to make the bar chart for the mesh\n",
    "    mesh_time_bar = np.zeros((len(resolutions_to_test), ))\n",
    "    mesh_resolution_bar = np.zeros((len(resolutions_to_test), ))\n",
    "\n",
    "    for i, resolution in enumerate(mesh_times):\n",
    "\n",
    "        mesh_time_bar[i] = resolution.time\n",
    "        mesh_resolution_bar[i] = f'{resolution.name}'\n",
    "\n",
    "    #Bar chart for the time taken to generate mesh\n",
    "    fig, mesh_time_chart = plt.subplots(2,2, tight_layout = True)\n",
    "\n",
    "    mesh_time_chart[0,0].bar(mesh_resolution_bar, mesh_time_bar, color = 'lightgray', edgecolor = 'lightgray')\n",
    "    mesh_time_chart[0,0].plot(mesh_resolution_bar, mesh_time_bar, c='r', linestyle = '-')\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    mesh_time_chart[0,0].set_ylabel('s')\n",
    "    mesh_time_chart[0,0].set_title('Generation Time')\n",
    "\n",
    "    #Plotting quadratic relation on top of the histogram for comparison. Disable new extra y-axis values\n",
    "    mesh_time_square = mesh_time_chart[0,0].twinx()\n",
    "    mesh_time_square.plot(mesh_resolution_bar, mesh_resolution_bar**3, c='g', linestyle = '--')\n",
    "    mesh_time_square.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    #Plot for amount of cells, in 1000s of cells\n",
    "    mesh_time_chart[0,1].bar(mesh_resolution_bar, mesh_test_cells, color = 'lightgray', edgecolor = 'lightgray')\n",
    "    mesh_time_chart[0,1].plot(mesh_resolution_bar, mesh_test_cells, c='r', linestyle = '-')\n",
    "\n",
    "    mesh_time_chart[0,1].set_ylabel('Cells * $10^3$')\n",
    "    mesh_time_chart[0,1].set_title('Amount of Cells')\n",
    "\n",
    "    mesh_cell_square = mesh_time_chart[0,1].twinx()\n",
    "    mesh_cell_square.plot(mesh_resolution_bar, mesh_resolution_bar**3, c='g', linestyle = '--')\n",
    "    mesh_time_square.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    #Plot for amount of vertices, in 1000s of vertices\n",
    "    mesh_time_chart[1,0].bar(mesh_resolution_bar, mesh_test_vertices, color = 'lightgray', edgecolor = 'lightgray')\n",
    "    mesh_time_chart[1,0].plot(mesh_resolution_bar, mesh_test_vertices, c='r', linestyle = '-')\n",
    "\n",
    "    mesh_time_chart[1,0].set_ylabel('Vertices * $10^3$')\n",
    "    mesh_time_chart[1,0].set_title('Amount of Vertices')\n",
    "\n",
    "    mesh_vertex_square = mesh_time_chart[1,0].twinx()\n",
    "    mesh_vertex_square.plot(mesh_resolution_bar, mesh_resolution_bar**3, c='g', linestyle = '--')\n",
    "    mesh_time_square.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    #Plot for smallest diameter, rescaled by the total domain size\n",
    "\n",
    "    mesh_time_chart[1,1].bar(mesh_resolution_bar, mesh_test_minimum_diameter, color = 'lightgray', edgecolor = 'lightgray')\n",
    "    mesh_time_chart[1,1].plot(mesh_resolution_bar, mesh_test_minimum_diameter, c='r', linestyle = '-')\n",
    "\n",
    "    mesh_time_chart[1,1].set_ylabel('$d_{min}/d_{tot}$')\n",
    "    mesh_time_chart[1,1].set_title('Smallest Cell Diameter')\n",
    "\n",
    "    mesh_diameter_square = mesh_time_chart[1,1].twinx()\n",
    "    mesh_diameter_square.plot(mesh_resolution_bar, mesh_resolution_bar**(-1.5), c='g', linestyle = '--')\n",
    "    mesh_time_square.axes.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Defining extremes of main diagonal to obtain side_length*3 cube\n",
    "# box_edge_low = Point(-domain_size/sqrt(2)*np.array([1,1,1]))\n",
    "# box_edge_high = Point(domain_size/sqrt(2)*np.array([1,1,1]))\n",
    "\n",
    "# #Using dolfin builtin mesh, defining edges and resolution for all 3 axes\n",
    "# cubic_mesh = BoxMesh(comm, box_edge_low, box_edge_high, mesh_resolution, mesh_resolution, mesh_resolution)\n",
    "\n",
    "# #Object returned by BoxMesh is not a dolfin mesh, so we define it as such to be able to use it\n",
    "# cubic_mesh = Mesh(cubic_mesh)\n",
    "\n",
    "# mesh = cubic_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh_partitioned = MeshPartitioning\n",
    "\n",
    "# distributed_mesh = mesh_partitioned.build_distributed_mesh(mesh)\n",
    "\n",
    "# print(mesh.mpi_comm())\n",
    "\n",
    "mesh.bounding_box_tree().compute_first_entity_collision(Point(domain_size, domain_size/2, domain_size/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plotting_option:\n",
    "\n",
    "        #Making a regular fine mesh(4 times as fine as initial unrefined mesh) to interpolate our functions over\n",
    "        cubic_mesh = make_cube_mesh(2*domain_size, plot_resolution*2)\n",
    "\n",
    "        #Getting a spherical mesh from the cube generated above, Making mesh slightly larger so we don't miss points \n",
    "        #on the surface that are absent due to the new mesh having a smooth boundary\n",
    "        mesh_for_plots = circle_grid_mesh(cubic_mesh, mesh_plot_size, show_mesh=False)\n",
    "\n",
    "#Creating a scalar function space on the large mesh, so we can interpolate functions on it and plot them\n",
    "# V_plot = FunctionSpace(mesh_for_plots, 'CG', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining coordinates for some test mass distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For all the points to be within a given radius, each coordinate must be smaller than\n",
    "#radius_population/sqrt(3)\n",
    "random_max_distance = radius_population/sqrt(3)\n",
    "\n",
    "#Setting a given seed so we can always have the same random numbers for now\n",
    "np.random.seed(1)\n",
    "\n",
    "#We want a mean of 0 so center of mass is in center, and the same standard deviation as the gaussian\n",
    "#pulse. This means we sample from the same distribution as the smooth one, and have the same mean.\n",
    "#This is exactly what we want to compare coarse and smooth distributions\n",
    "mu, sigma = 0, stand_dev\n",
    "random_coordinates_x = np.random.normal(mu, sigma, source_number)\n",
    "random_coordinates_y = np.random.normal(mu, sigma, source_number)\n",
    "\n",
    "#If we want all source to be in the same plane, we set the z axis to be 0 for all of them. Otherwise,\n",
    "#random as above\n",
    "if coplanar_sources == True:\n",
    "    \n",
    "    random_coordinates_z = np.zeros((source_number, 1)).ravel()\n",
    "\n",
    "else:\n",
    "    \n",
    "    random_coordinates_z = np.random.normal(mu, sigma, source_number)\n",
    "\n",
    "#If we dont need Gaussian, defining a source_number*3 array of random numbers between 0 and 1 and\n",
    "#multiplying by the radius just defined so all points are inside a sphere of radius_tot.\n",
    "#Subtracting 0.5 so #we're sampling equally from the positive and negative instead of from 0 to 1\n",
    "# random_coordinates = random_max_distance * (np.random.rand(source_number, 3)-0.5)\n",
    "\n",
    "# # random_coordinates[0][0] = -domain_size/their_distance\n",
    "if central_mass:\n",
    "\n",
    "    random_coordinates_x[0] = 0\n",
    "    random_coordinates_y[0] = 0\n",
    "    random_coordinates_z[0] = 0\n",
    "    \n",
    "#     # Uncomment for test case with two equal masses on the xy plane at a given distance\n",
    "#     their_distance = 2\n",
    "    \n",
    "#     random_coordinates_x[1] = -domain_size/their_distance\n",
    "#     random_coordinates_y[1] = 0\n",
    "#     random_coordinates_z[1] = 0\n",
    "    \n",
    "#     random_coordinates_x[2] = domain_size/their_distance\n",
    "#     random_coordinates_y[2] = 0\n",
    "#     random_coordinates_z[2] = 0\n",
    "\n",
    "#Overall array containing all coordinates. If over-writing the random position, this has to go after it,\n",
    "#otherwise the c++ array for the source sets the wrong position!\n",
    "random_coordinates = np.array((random_coordinates_x, random_coordinates_y, random_coordinates_z))\n",
    "random_coordinates = np.transpose(random_coordinates)\n",
    "\n",
    "#Obtaining the center of each source as a list of points\n",
    "source_centers = [Point(random_coordinates_x[i], random_coordinates_y[i], random_coordinates_z[i]) for i in range(source_number)]\n",
    "\n",
    "# Overriding definition with known point for testing of the mesh refinement\n",
    "# test_coordinates  = 5*kp*np.zeros((3,1))\n",
    "# source_centers = Point(test_coordinates)\n",
    "\n",
    "# random_coordinates\n",
    "# print(f'Mean in x: {abs(mu - np.mean(random_coordinates_x))/domain_size}\\n') \n",
    "# print(f'Mean in y: {abs(mu - np.mean(random_coordinates_y))/domain_size}\\n') \n",
    "# print(f'Mean in z: {abs(mu - np.mean(random_coordinates_z))/domain_size}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Random coordinates are:\\n {random_coordinates}')\n",
    "# print(source_centers[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for point in source_centers:\n",
    "    \n",
    "#     print(f'{point.x(), point.y(), point.z()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the center of mass for the mass distribution to correctly calculate BCs and initial guesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENTED OUT MOMENTARILY TO TEST HAVING SOURCE IN PLANE!\n",
    "#NEED TO TEST IF THIS WORKS CORRECTLY WITH RANDOM POINTS. PRROBLEM BEOFR EWAS WITH MESH\n",
    "#REFINEMENT, WASNT READING THE OOINT TO REFINE CORRECTLY!\n",
    "\n",
    "#For the current case in which all sources have the same mass, we simply divide by #sources\n",
    "center_of_mass_x = random_coordinates[:,0].sum()/source_number\n",
    "center_of_mass_y = random_coordinates[:,1].sum()/source_number\n",
    "center_of_mass_z = random_coordinates[:,2].sum()/source_number\n",
    "\n",
    "#Overall center of mass\n",
    "center_of_mass = [center_of_mass_x, center_of_mass_y, center_of_mass_z]\n",
    "\n",
    "# center_of_mass = test_coordinates\n",
    "# center_of_mass_x = center_of_mass[0]\n",
    "# center_of_mass_y = center_of_mass[1]\n",
    "# center_of_mass_z = center_of_mass[2]\n",
    "\n",
    "# # #Overwriting center of mass to check if the BC works correctly\n",
    "# # center_of_mass = [0,0,0]\n",
    "# source_centers[0]/kp\n",
    "\n",
    "# center_of_mass\n",
    "print(f'Process {rank} about to refine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Starting mesh refinement...\\n')\n",
    "mesh_refine_start = time.time()\n",
    "new_mesh = more_modified_refinement(mesh, source_centers, refine_times)\n",
    "# new_mesh = local_refinement(mesh, source_centers, radius_refine, refine_times, technique = 'ring')\n",
    "mesh_refine_end = time.time()\n",
    "mesh_refine_time = run_time(mesh_refine_end - mesh_refine_start, 'Mesh Refinement')\n",
    "section_times.append(mesh_refine_time)\n",
    "print('Mesh refined in {} s \\n'.format(mesh_refine_time.time))\n",
    "\n",
    "#Saving the mesh to a file so we can then retrieve it from all processes\n",
    "# mesh_file = File('mesh.xml')\n",
    "# mesh_file << new_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to save a mesh to a file and then load it from each process so there's no need to broadcast it!\n",
    "#Doesnt seem to work for now though\n",
    "# mesh_from_file = Mesh('mesh.xml')\n",
    "# mesh = mesh_from_file\n",
    "\n",
    "mesh = new_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering all the data from the mesh AFTER having done the mesh refinement and defined the mesh for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh.mpi_comm = comm\n",
    "# mesh.mpi_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rearranging mesh data\\n')\n",
    "rearrange_start = time.time()\n",
    "\n",
    "#IMPORTANT: Tried solving the whole PDE with degree 2, so that taking the Laplacian of the solution would\n",
    "#give a smoother expression. However, that did not work for mesh resolution = 21 ,it takes up too much\n",
    "#memory and cant be run. Can solve up to degree 3 but with low resolution = 11. Can't do degree = 4!\n",
    "#The alternative is to take the solution itself and take its Laplacian and project it on a degree 3 space\n",
    "#Calling the rearrange_mesh_data function to get coordinates and order them based on the \n",
    "#distance from the center of mass\n",
    "V, vertex_number, x_coords, y_coords, z_coords, r_coords, sorting_index, x_sorted, y_sorted, z_sorted, r_sorted = rearrange_mesh_data(mesh, center_of_mass, degree_PDE)\n",
    "\n",
    "#To be able to gather the coordinate arrays with MPI, the coordinates need to be C_contiguous\n",
    "x_coords, y_coords, z_coords, r_coords = [np.ascontiguousarray(coord_array) for coord_array in [x_coords, y_coords, z_coords, r_coords]]\n",
    "\n",
    "rearrange_end = time.time()\n",
    "rearrange_time = run_time(rearrange_end - rearrange_start, 'Mesh data rearrange')\n",
    "section_times.append(rearrange_time)\n",
    "print('Mesh data rearranged in {} s \\n'.format(rearrange_time.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_string = make_discrete_gauss(10)\n",
    "# print(f'The generated string is:\\n {test_string},\\n The normal string is:\\n {f_exponent_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few BVP from combinations we use often. Naming scheme: 'weak form_source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BVPs for a discrete dirac mass distribution, for Newton and MOND with/out interpolations\n",
    "newton_dirac = BVP(F_Newton, u_Newton, f_multiple_dirac, 'Newton, discrete dirac')\n",
    "mond_deep_dirac = BVP(F_MOND_deep, u_displaced_cpp, f_multiple_dirac, 'Deep MOND, discrete dirac')\n",
    "mond_simple_dirac = BVP(F_MOND_simple, u_displaced_cpp, f_multiple_dirac, 'Simple MOND, discrete dirac')\n",
    "mond_standard_dirac = BVP(F_MOND_standard, u_displaced_cpp, f_multiple_dirac, 'Standard MOND, discrete dirac')\n",
    "\n",
    "#BVPs for a discrete gauss mass distribution.\n",
    "newton_gauss = BVP(F_Newton, u_Newton, f_multiple_gauss, 'Newton, discrete gauss')\n",
    "mond_deep_gauss = BVP(F_MOND_deep, u_displaced_cpp, f_multiple_gauss, 'Deep MOND, discrete gauss')\n",
    "mond_simple_gauss = BVP(F_MOND_simple, u_displaced_cpp, f_multiple_gauss, 'Simple MOND, discrete gauss')\n",
    "mond_standard_gauss = BVP(F_MOND_standard, u_displaced_cpp, f_multiple_gauss, 'Standard MOND, discrete gauss')\n",
    "\n",
    "#BVPs for a continuous distribution, for Newton and MOND with/out interpolations\n",
    "newton_continuous = BVP(F_Newton, u_Newton, f_exponent_test, 'Newton, continuous gauss')\n",
    "mond_deep_continuous = BVP(F_MOND_deep, u_displaced_cpp, f_exponent_test, 'Deep MOND, continuous gauss')\n",
    "mond_simple_continuous = BVP(F_MOND_simple, u_displaced_cpp, f_exponent_test, 'Simple MOND, continuous gauss')\n",
    "mond_standard_continuous = BVP(F_MOND_standard, u_displaced_cpp, f_exponent_test, 'Standard MOND, continuous gauss')\n",
    "\n",
    "#BVPs for a three parameter beta distribution\n",
    "newton_beta = BVP(F_Newton, u_Newton, f_gas_three_beta, 'Newton, beta')\n",
    "mond_deep_beta = BVP(F_MOND_deep, u_sphere_cpp, f_gas_three_beta, 'Deep MOND, beta')\n",
    "mond_simple_beta = BVP(F_MOND_simple, u_sphere_cpp, f_gas_three_beta, 'Simple MOND, beta')\n",
    "mond_standard_beta = BVP(F_MOND_standard, u_sphere_cpp, f_gas_three_beta, 'Standard MOND, beta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking parameters from the cluster database of Reiprich and Moffat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the measurements use a reduced Hubble constant. As the Hubble constant can be expressed in terms\n",
    "#of Miglrom's constant and speed of light, we express it as that. I modified a0 = 1.14 * 10**-10 so \n",
    "#that H0 is now very close to the latest (upper) estimate. Now a0 and H0 are consistent.\n",
    "H0 = 2*pi*(a0)/c\n",
    "\n",
    "#The Hubble constant used is h50, given as H0/50 km/s/Mpc. We need to multiply most of the quantities in\n",
    "#the table by this to obtain \n",
    "h50 = H0/(50*10**3/(1000*kp))\n",
    "\n",
    "#In the database, there is an optional parameter c that indicates a different way of calculating the\n",
    "#error. Only some lines have it, so it throws pandas off. Removed it in the file itself\n",
    "#a2065, a2063, ngc5846 are missing from reiprich 2001 for rho_0. Had to add them in by hand \n",
    "#in the rho_0 file.\n",
    "#In the PhD thesis Reiprich cut the table wrong so they're invisible! Unbelievable\n",
    "\n",
    "#Importing the text file\n",
    "df = pd.read_csv('cluster_database/cluster_data.txt', delimiter='\\s+', header = None)\n",
    "\n",
    "#As the columns have no names in the initial file, we add the names here\n",
    "df.columns = ['name', 'beta_frame', 'beta+', 'beta-', 'r_c_frame', 'r_c+', 'r_c-', 'T', 'T+', 'T-',\n",
    "              'm5_frame', 'm5+', 'm5-', 'r5', 'r5+', 'r5-', 'm2', 'm2+', 'm2-', 'r2', 'r2+', 'r2-',\n",
    "              'mtot_frame', 'ref']\n",
    "\n",
    "# Using readline to open file containing rho_0 values and storing each line in Lines\n",
    "file1 = open('cluster_database/rho_0.txt', 'r') \n",
    "Lines = file1.readlines() \n",
    "file1.close()\n",
    "\n",
    "#Initialising numpy array to hold the values of rho_0\n",
    "rho_0_np = np.zeros((len(Lines),1))\n",
    "\n",
    "#Storing the content of each line into the numpy array\n",
    "for i, line in enumerate(Lines):\n",
    "    \n",
    "    #assigning each line to a member of the rho_0 numpy array\n",
    "    rho_0_np[i] = line\n",
    "\n",
    "#Flattening the array before putting it in the dataframe    \n",
    "rho_0_np = rho_0_np[:,0]\n",
    "#Adding the rho_0 values to the dataframe\n",
    "df['rho_0_frame'] = pd.Series(rho_0_np, index = df.index)\n",
    "\n",
    "# Displaying the full, uncut database with all parameters\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(df.head(2))\n",
    "    \n",
    "#The radii are given in units of kpc/h50. To get them in kpc we need to divide by h50\n",
    "#Only one useful for now to be scaled is r_c\n",
    "df.loc[:, 'r_c_frame'] = df.loc[:, 'r_c_frame']/h50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to implement this itnegral directly in the Running_Python notebook to figure out what the total\n",
    "#mass should be!\n",
    "\n",
    "rho_0*r_c*atan(domain_size/r_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying an alternative method for assigning values inside the c++ expressions by using exec, to avoid the limit on eval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_string = make_source_string(10)\n",
    "# test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function for the boundary. Since we only have one BC for the whole boundary, we\n",
    "#can make a simple function that returns true for each value on the boundary\n",
    "#the on_boundary built-in function takes each point in domain and returns true if on boundary\n",
    "def boundary(x, on_boundary):\n",
    "    return on_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_PDE(the_BVP):\n",
    "    '''Function takes in a BVP object, which defines the weak form, initial guess/BC and source for\n",
    "    a PDE, and computes its solution'''\n",
    "    \n",
    "    ## starting time of PDE solver\n",
    "    solver_start = time.time()\n",
    "    print('Starting PDE Solver...\\n')\n",
    "\n",
    "    #defining the x,y,z coordinates from the coordinate array in sympy\n",
    "#     x, y, z = sym.symbols('x[0], x[1], x[2]')\n",
    "    \n",
    "    #Modifying the total mass to that of the beta distribution if the BVP contains beta\n",
    "#     if 'beta' in the_BVP.name:\n",
    "        \n",
    "#         mgb = (np.trapz(rho_0/(1+(r_sorted/r_c)**2)**(3*beta/2)*4*pi*r_sorted**2,\n",
    "#                       x = r_sorted))\n",
    "    \n",
    "#     print(f'mass/mass_coma is {mgb/mass_coma_gas}')\n",
    "    \n",
    "    #VERY IMPORTANT: If using sympy, use sym. in front of every mathematical operator, or the sym. and UFL (the\n",
    "    #mathematical syntax used in fenics) collide and an error about UFL conversion appears\n",
    "    \n",
    "    #Defining the source term here, cause the make_source_string function creates a string that \n",
    "    #evaluate the expression for a variable called 'source'\n",
    "    source = the_BVP.source\n",
    "    \n",
    "    #Evaluating the source term obtained from the make_source_string function\n",
    "    f = eval(make_source_string(source_number))\n",
    "    \n",
    "    #Declaring the expression for the initial guess\n",
    "    u = (Expression(the_BVP.initial_guess,\n",
    "    degree = degree_PDE, a0 = a0, ms = ms,mgb = mgb, G = G,  ly = ly, kp = kp, radius_tot = radius_tot,\n",
    "    volume_out = volume_out, center_of_mass_x = center_of_mass_x,\n",
    "    center_of_mass_y = center_of_mass_y, center_of_mass_z = center_of_mass_z,\n",
    "    source_number = source_number, source_mass = source_mass))\n",
    "\n",
    "    #Declaring the expression for the boundary condition with displaced CM (center of mass)\n",
    "    boundary_CM = u\n",
    "\n",
    "    #Declaring the boundary condition. It takes three arguments: function space, value of BC, \n",
    "    #section of the boundary (in our case the whole boundary).\n",
    "    bc = DirichletBC(V, boundary_CM, boundary)\n",
    "\n",
    "    #Defining the variational problem\n",
    "    #u is the solution. for linear problems, we'd have to define it as TrialFunction, but for \n",
    "    #non-linear we define it as Function directly\n",
    "    u = interpolate(u, V)\n",
    "\n",
    "    #defining the test function\n",
    "    v = TestFunction(V)\n",
    "\n",
    "    #defining the weak form to be solved\n",
    "    F = eval(the_BVP.weak_form)\n",
    "\n",
    "    #Computing the solution for normal deep MOND\n",
    "    (solve(F == 0, u, bc, solver_parameters={\"newton_solver\":{\"relative_tolerance\":1e-6},\n",
    "                                             \"newton_solver\":{\"maximum_iterations\":200}}))\n",
    "\n",
    "    solver_end = time.time()\n",
    "    solver_time = run_time(solver_end - solver_start, 'PDE Solver')\n",
    "    section_times.append(solver_time)\n",
    "\n",
    "    print('PDE solved in {}\\n'.format(solver_time.time))\n",
    "    \n",
    "    return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Waiting for each process to have completed before moving on to solve the PDE\n",
    "# MPI.barrier(comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined the quantity BVP_to_solve in the MONDquantities file as a string, so to use it we need to \n",
    "#evaluate it with eval.\n",
    "u, f = solve_PDE(eval(BVP_to_solve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "## First, the potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to set x,y,z axes labels and ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the values of the function, its gradient and the source at each vertex of the mesh, and the coordinates at each point of the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection_start = time.time()\n",
    "print('Collecting data from PDE...\\n')\n",
    "\n",
    "if plotting_option == True:\n",
    "    mesh = mesh_for_plots\n",
    "    V_plot, vertex_number, x_coords, y_coords, z_coords, r_coords, sorting_index, x_sorted, y_sorted, z_sorted, r_sorted = rearrange_mesh_data(mesh, center_of_mass)\n",
    "    u_plot = interpolate(u, V_plot)\n",
    "    f_plot = interpolate(f, V_plot)\n",
    "    u = u_plot\n",
    "    #Calling the rearrange_mesh_data function to get coordinates and order them based on the \n",
    "    #distance from the center of mass\n",
    "\n",
    "#The value of the function at each vertex of the mesh is stored in a np array. Its order\n",
    "#corresponds to the otder of the mesh.coordinates() values\n",
    "potential = u.compute_vertex_values()\n",
    "\n",
    "#The value of the source at each vertex of the mesh\n",
    "source = 1/(4*pi*G)*f.compute_vertex_values(mesh)\n",
    "\n",
    "#Getting the degree from the scalar function space V from the PDE\n",
    "degree = V.ufl_element().degree()\n",
    "\n",
    "#Laplacian of the solution to get back the scaled mass distribution\n",
    "lap = div(grad(u))\n",
    "\n",
    "apparent_mass_project = project(lap, V)\n",
    "\n",
    "if lensing_interpolations:\n",
    "\n",
    "    #For integration, we can only act on (scalar) FunctionSpace. Hence, we project each derivative\n",
    "    #of u individually onto the same function space as u\n",
    "    #If having problem with space, check the page that suggested this\n",
    "    #https://fenicsproject.org/qa/3688/derivative-in-one-direction/\n",
    "    acceleration_project_vector = [project(u.dx(i), V) for i in range(3)]\n",
    "\n",
    "    #Declaring a tensor(matrix) function space onto which to project the Jacobian of the potential\n",
    "    T = TensorFunctionSpace(mesh, 'P', degree)\n",
    "\n",
    "    #Projecting the gradient of the gradient (jacobian) of the potential onto TensorFunctionSpace\n",
    "    lensing_jacobian_project = project(grad(acceleration_project), T)\n",
    "\n",
    "    #First, getting each element of the lensing jacobian into a 1*9 list by differentiating each\n",
    "    #element of the acceleration list w.r.t. each coordinate. List is handy to loop over\n",
    "    #with only one index, e.g. to integrate or graph\n",
    "    lensing_jacobian_project_list = ([project(acceleration_project_vector[i].dx(j), V) \n",
    "                                        for i in range(3) for j in range(3)])\n",
    "\n",
    "    #Next, reshaping the list into a 3x3 array so we can access it by pair of coordinates\n",
    "    lensing_jacobian_project_matrix = np.reshape(lensing_jacobian_project_list,(3,3))\n",
    "\n",
    "    #Putting the values of the Jacobian into a np array\n",
    "    lensing_jacobian_magnitude = lensing_jacobian_project.compute_vertex_values()\n",
    "\n",
    "    #Reshaping the lensing jacobian into a matrix of vectors, an order 3 tensor, to have the value\n",
    "    #the jacobian at each point in space, like with the acceleration\n",
    "    lensing_jacobian_magnitude = (np.reshape(lensing_jacobian_magnitude, (3, 3,\n",
    "                                             int(lensing_jacobian_magnitude.shape[0]/9))))    \n",
    "\n",
    "#Projecting the acceleration onto a vector space is expensive, so don't do it unless needed\n",
    "#If not needed, set acceleration to 0 everywhere\n",
    "if acceleration_needed:\n",
    "    \n",
    "    #To obtain the values for the acceleration, we need to define a new function space, since the \n",
    "    #gradient is a vector function is the function space for the PDE is a scalar function space\n",
    "    W = VectorFunctionSpace(mesh, 'P', degree)\n",
    "    \n",
    "    #Projecting (similar to interpolating) the grad(u) field onto W, gives a function\n",
    "    acceleration_project = project(grad(u), W)\n",
    "\n",
    "    #The result of project is n*3,1 np.array, with 3 (x,y,z) values for each of the n vertices\n",
    "    acceleration = acceleration_project.compute_vertex_values()\n",
    "    \n",
    "    #reshaping the array to split the x,y,z components into their own column each\n",
    "    acceleration = np.reshape(acceleration, (3, int(acceleration.shape[0]/3)))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    acceleration = np.zeros((3, len(potential)))\n",
    "    \n",
    "\n",
    "acceleration_x = acceleration[0]\n",
    "acceleration_y = acceleration[1]\n",
    "acceleration_z = acceleration[2]\n",
    "\n",
    "#Finding the magnitude of the acceleration\n",
    "acceleration_magnitude = np.linalg.norm(acceleration, axis=0)\n",
    "\n",
    "#Sorting the potential, acceleration and source according to thr r of the vertex they pertain to\n",
    "potential_sorted = potential[sorting_index]\n",
    "acceleration_magnitude_sorted = acceleration_magnitude[sorting_index]\n",
    "source_sorted = source[sorting_index]\n",
    "\n",
    "data_collection_end = time.time()\n",
    "data_collection_time = run_time(data_collection_end - data_collection_start, 'Data Collection')\n",
    "section_times.append(data_collection_time)\n",
    "print('Data collected in {} s\\n'.format(data_collection_time.time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Laplacian of the potential to obtain the apparent dark matter distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The apparent mass distribution is the RHS of the Newtonian Poisson equation. No need to scale it as it\n",
    "#is already scaled in the expression for the source itself\n",
    "# apparent_mass_divergence = div(acceleration_project)\n",
    "\n",
    "# #Projecting the divergence above onto the same scalar function space as the potential\n",
    "# apparent_mass_project = project(apparent_mass_divergence, V)\n",
    "\n",
    "integral = assemble(lap*dx)\n",
    "\n",
    "#Gathering the values of the mass distribution \n",
    "apparent_mass_distribution = 1/(4*pi*G)*apparent_mass_project.compute_vertex_values()\n",
    "\n",
    "#Sorting the mass distribution values\n",
    "apparent_mass_distribution_sorted = apparent_mass_distribution[sorting_index]\n",
    "\n",
    "(integral/(4*pi*G))/mgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering the potential and coordinate numpy array onto process 0 to have the full solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need to know how many vertices we have in total in the full mesh to preallocate the array\n",
    "#for both the potential and the coordinates. We do this with the MPI reduce operation MPI_SUM\n",
    "print(f'Process {rank}: potential has {len(potential)} elements.')\n",
    "\n",
    "#We need the total #vertices as an int to define an array. Calling MPI.sum with communicator and\n",
    "#value to be summed from each process\n",
    "total_mesh_vertices = int(MPI.sum(comm, len(potential)))\n",
    "\n",
    "if rank == 0:\n",
    "\n",
    "    print(f'Process {rank}: the overall potential has {total_mesh_vertices} elements.')\n",
    "\n",
    "#Now we can gather all values of the potential and coordinates. First, we define arrays to hold the\n",
    "#result, the size of the total potential on process 0\n",
    "\n",
    "#Have to initialise the receving buffer for the potential to None on all processes or we get an error\n",
    "potential_total = None\n",
    "x_coords_total = None\n",
    "y_coords_total = None\n",
    "z_coords_total = None\n",
    "r_coords_total = None\n",
    "source_total = None\n",
    "apparent_mass_total = None\n",
    "\n",
    "if rank == 0:\n",
    "    \n",
    "    #There is a problem with the receive buffer not being big enough. A simple fix for now is to \n",
    "    #multiply its size by 1.5, then we can remove all the trailing zeros\n",
    "    receiver_size = int(1.5*total_mesh_vertices)\n",
    "\n",
    "    potential_total = np.empty(receiver_size, dtype = type(potential[0]))\n",
    "    x_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "    y_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "    z_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "    r_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "    source_total = np.empty(receiver_size, dtype = type(potential[0]))\n",
    "    apparent_mass_total = np.empty(receiver_size, dtype = type(potential[0]))\n",
    "    \n",
    "#IMPORTANT: Have to use Gatherv, not Gather, or it won't work!\n",
    "comm.Gatherv(potential, potential_total, root = 0)\n",
    "comm.Gatherv(x_coords, x_coords_total, root = 0)\n",
    "comm.Gatherv(y_coords, y_coords_total, root = 0)\n",
    "comm.Gatherv(z_coords, z_coords_total, root = 0)\n",
    "comm.Gatherv(r_coords, r_coords_total, root = 0)\n",
    "comm.Gatherv(source, source_total, root = 0)\n",
    "comm.Gatherv(apparent_mass_distribution, apparent_mass_total, root = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to sort as usual, now for the total potential and based on the overall r coordinates\n",
    "\n",
    "if rank == 0:\n",
    "\n",
    "    #Storing the index to sort according to the total r\n",
    "    sorting_index_total = r_coords_total.argsort()\n",
    "\n",
    "    #Sorting all total quantities\n",
    "    r_total_sorted = r_coords_total[sorting_index_total]\n",
    "    \n",
    "    x_total_sorted = x_coords_total[sorting_index_total]\n",
    "    \n",
    "    y_total_sorted = y_coords_total[sorting_index_total]\n",
    "    \n",
    "    z_total_sorted = z_coords_total[sorting_index_total]\n",
    "    \n",
    "    potential_total_sorted = potential_total[sorting_index_total]\n",
    "    \n",
    "    source_total_sorted = source_total[sorting_index_total]\n",
    "    \n",
    "    apparent_mass_total_sorted = apparent_mass_total[sorting_index_total]\n",
    "    \n",
    "    #Finding the zero elements in the sorted r array, and removing them. We do this by only keeping the\n",
    "    #Finding the indices for which r is larger than the smallest r on process 0, divided by 10**5 just to\n",
    "    #make sure. There should definitely not be any mesh points with distances smaller than that!\n",
    "    total_nonzero_indices = (r_total_sorted > r_sorted[0]/(10**5))\n",
    "    \n",
    "    #Taking the non-padding components of radius, potential, source and mass distribution\n",
    "    x_total_sorted = x_total_sorted[total_nonzero_indices]\n",
    "    y_total_sorted = y_total_sorted[total_nonzero_indices]\n",
    "    z_total_sorted = z_total_sorted[total_nonzero_indices]\n",
    "    r_total_sorted = r_total_sorted[total_nonzero_indices]\n",
    "    potential_total_sorted = potential_total_sorted[total_nonzero_indices]\n",
    "    source_total_sorted = source_total_sorted[total_nonzero_indices]\n",
    "    apparent_mass_total_sorted = apparent_mass_total_sorted[total_nonzero_indices]\n",
    "    \n",
    "    #The dark matter is the difference between the apparent and source masses\n",
    "    dark_mass_total_sorted = apparent_mass_total_sorted - source_total_sorted\n",
    "    \n",
    "    #Saving all these numpy arrays so we can plot them again in Python, instead of just having a saved figure\n",
    "    #that is not interactive!\n",
    "    \n",
    "    #Declaring all the temporary files we want to write to\n",
    "#     potential_saved = TemporaryFile()\n",
    "#     source_saved = TemporaryFile()\n",
    "#     apparent_mass_saved = TemporaryFile()\n",
    "#     r_sorted_saved = TemporaryFile()\n",
    "    \n",
    "    #Saving all the quantities to the respective files\n",
    "    np.save('Numpy_Arrays/potential_saved.npy', potential_total_sorted)\n",
    "    np.save('Numpy_Arrays/source_saved.npy', source_total_sorted)\n",
    "    np.save('Numpy_Arrays/apparent_mass_saved.npy', apparent_mass_total_sorted)\n",
    "    np.save('Numpy_Arrays/dark_mass_saved.npy', dark_mass_total_sorted)\n",
    "    np.save('Numpy_Arrays/x_sorted_saved.npy', x_total_sorted)\n",
    "    np.save('Numpy_Arrays/y_sorted_saved.npy', y_total_sorted)\n",
    "    np.save('Numpy_Arrays/z_sorted_saved.npy', z_total_sorted)\n",
    "    np.save('Numpy_Arrays/r_sorted_saved.npy', r_total_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "\n",
    "    fig, fig_total_potential = plt.subplots(sharex=True, sharey=True)\n",
    "\n",
    "    fig_total_potential.scatter(x_total_sorted, potential_total_sorted, marker = '.', s = 0.5, c = y_total_sorted/y_total_sorted.max(), cmap = 'jet')\n",
    "    \n",
    "#     plot_annotations(fig_total_potential)\n",
    "\n",
    "    #Formatting plot using the function I made\n",
    "    plot_format(fig_total_potential,1,0)\n",
    "    \n",
    "    #Saving the figure in the Figure folder, removed padding arounf with bbox_inches and \n",
    "    plt.savefig(f'Figures/total_potential.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "\n",
    "    print(f'potential_total has length {len(potential_total)}')\n",
    "    \n",
    "    potential_total_no_zeros = potential_total[np.nonzero(potential_total)]\n",
    "\n",
    "    print(f'potential_total_no_zeros has length {len(potential_total_no_zeros)}')\n",
    "    \n",
    "    \n",
    "    print(f'x_coords has length: {len(x_coords)}')\n",
    "    print(f'x_coords_total has length: {len(x_coords_total)}')\n",
    "    # x_coords_total\n",
    "    x_total_no_zeros = x_coords_total[np.nonzero(x_coords_total)]\n",
    "    print(f'x_total_no_zeros has length: {len(x_total_no_zeros)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering the apparent_mass distribution onto process 0 exactly as we did for potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating quantitites along a straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if lensing_interpolations:\n",
    "    \n",
    "    #Defining starting and ending point for the integration, at two ends of a line on xy-plane\n",
    "    #Multiplying vertices by 0.99 to avoid asking for points outside the domain \n",
    "    starting_point = np.array([0, 0, domain_size*0.99])\n",
    "    ending_point = np.array([0, 0, -domain_size*0.99])\n",
    "\n",
    "    #using the line_integral function defined above\n",
    "    potential_integrated = line_integral(u, starting_point, ending_point, n=100)\n",
    "\n",
    "    #Integrating all components of the lensing Jacobian\n",
    "    lensing_jacobian_integrated_list = ([line_integral(component,starting_point,ending_point,n=100)\n",
    "                                   for component in lensing_jacobian_project_list])\n",
    "\n",
    "    #Putting the list in matrix form, can be handier to access\n",
    "    lensing_jacobian_integrated_matrix = np.reshape(lensing_jacobian_integrated_list,(3,3))\n",
    "\n",
    "    #We only need the Jacobian in a plane. In our case, our integration path given by starting_point\n",
    "    #and ending_point is along the z axis, so we're interested in the xy Jacobian, [0:1,0:1]\n",
    "    lensing_jacobian_xy_plane = lensing_jacobian_integrated_matrix[0:2, 0:2]\n",
    "\n",
    "    #We now obtain the quantities needed for the lensing formalism as described at:\n",
    "    #https://en.wikipedia.org/wiki/Gravitational_lensing_formalism#Lensing_Jacobian\n",
    "\n",
    "    #First, the distance between observer and lens. We set the observer and source to be the \n",
    "    #starting and end_points\n",
    "    observer_lens_Dd = domain_size\n",
    "\n",
    "    #Then, distance between source and lens\n",
    "    source_lens_Dds = domain_size\n",
    "\n",
    "    #And the overall distance between source and observer\n",
    "    source_observer_Ds = observer_lens_Dd + source_lens_Dds\n",
    "\n",
    "    #Now we find the Jacobian in the xy plane\n",
    "    jacobian_xy_plane_A = (np.eye(2) - 2*source_lens_Dds/(observer_lens_Dd*source_observer_Ds*c**2)*\n",
    "                           lensing_jacobian_xy_plane)\n",
    "\n",
    "    #PROBLEM: The value of the Jacobian for the current values is too small, so subtracting the\n",
    "    #small number from 1 returns 1! Need to find a way of working with more precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting radially\n",
    "## First, the potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to compute the sum of the individual contributions from the analytic form so we can compare them to the overall solution we get from the PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_individual_diracs = 0\n",
    "\n",
    "for coordinates in random_coordinates:\n",
    "\n",
    "    #Obtaining the sorted coordinates for each source from random coordinates\n",
    "    V, vertex_number, x_coords, y_coords, z_coords, r_coords, sorting_index, x_sorted, y_sorted, z_sorted, r_sorted = rearrange_mesh_data(mesh, coordinates)\n",
    "\n",
    "    potential_individual_diracs = potential_individual_diracs + sqrt(G*mgb*a0)*np.log(r_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_individual_sum = sum_individual_contributions(mesh, origin, random_coordinates)\n",
    "\n",
    "type(potential_individual_sum)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# plt.scatter(x_coords, potential_individual_sum, s = 0.1)\n",
    "plt.scatter(x_coords, potential, s=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "radial_plots_start = time.time()\n",
    "\n",
    "#Defining analytic functions to check if the result is correct\n",
    "#2nd argument of Heaviside is its value where the Heaviside goes from 0 to 1\n",
    "potential_sphere_analytic = (np.heaviside(r_sorted - radius_tot, 0.5)*sqrt(G*mgb*a0)*np.log(r_sorted) +\n",
    "(np.heaviside(radius_tot - r_sorted, 0.5))*(4/3*sqrt(pi/3*a0*G*mgb/volume_out)*np.power(r_sorted,3/2)+\n",
    "sqrt(G*mgb*a0)*ln(radius_tot) - 4/3*sqrt(pi/3*a0*G*mgb/volume_out)*radius_tot**(3/2)))\n",
    "\n",
    "#Analytic potential on the inside of a sphere\n",
    "potential_inside_analytic = (4/3*sqrt(pi/3*a0*G*mgb/volume_out)*3/2*np.power(r_sorted,3/2)+\n",
    "sqrt(G*mgb*a0)*ln(radius_tot) - 4/3*sqrt(pi/3*a0*G*mgb/volume_out)*3/2*radius_tot**(3/2))\n",
    "\n",
    "#Analytic potential for a Dirac Delta\n",
    "potential_dirac_analytic = sqrt(G*mgb*a0)*np.log(r_sorted)\n",
    "\n",
    "#Analytic potential for multiple sources (scales with sqrt(#masses))\n",
    "potential_multiple_dirac_analytic = sqrt(G*mgb*a0/source_number)*np.log(r_sorted)\n",
    "\n",
    "#Analytic potentials for isothermal distribution\n",
    "potential_isothermal_analytic = 2/3*sqrt(G*mgb*a0/6)*np.log(1 + np.power(r_sorted, 3/2)/p**(3/2))\n",
    "\n",
    "#Plotting radial FEM solution and analytic solution on the same plot. We use subplots so'\n",
    "#we can put multiple axes on the same plot and plot different scales\n",
    "fig, potential1 = plt.subplots(sharex=True, sharey=True)\n",
    "\n",
    "color = 'tab:red'\n",
    "potential1.set_ylabel('FEM', color=color)\n",
    "\n",
    "potential1.plot(r_sorted, potential_sorted, label = 'FEM', color=color, linestyle='-')\n",
    "\n",
    "#Plotting the GEA potential as well\n",
    "# potential1.plot(r_sorted, potential_GEA_sorted, label = 'FEM_GEA', color='tab:green', linestyle='--', linewidth=0.5)\n",
    "\n",
    "potential1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#UNCOMMENT TO HAVE SEPARATE AXES TO COMPARE SHAPES\n",
    "# potential2 = potential1.twinx()\n",
    "color = 'tab:blue'\n",
    "# potential2.set_ylabel('Analytic', color=color)\n",
    "plt.plot(r_sorted, potential_multiple_dirac_analytic, label = 'Dirac Analytic', linestyle = '--', )\n",
    "# potential2.plot(r_sorted, potential_sphere_analytic, label = 'Dirac Analytic', linestyle = '--', color=color)\n",
    "\n",
    "#It is possible to use Latex directly in the labels by enclosing expressions in $$\n",
    "# plt.ylabel('$\\phi$')\n",
    "\n",
    "plot_annotations(potential1)\n",
    "\n",
    "#Formatting plot using the function I made\n",
    "plot_format(potential1,1,1)\n",
    "\n",
    "potential1_title = f'potential_1_p{rank}'\n",
    "\n",
    "#Saving the figure in the Figure folder, removed padding arounf with bbox_inches and. This is executed\n",
    "#by each process, so uncomment if need to see solution from each process separately\n",
    "# plt.savefig(f'Figures/{potential1_title}.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the error in the potential, radially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for spherically symmetric mass distributions we have the anlytic solution, so we can compute\n",
    "#the error\n",
    "potential_error = np.abs((potential_sorted - potential_dirac_analytic)/potential_sorted)\n",
    "potential_proportionality = potential_dirac_analytic/potential_sorted\n",
    "\n",
    "#plotting the error against the radius\n",
    "fig, plot_potential_error = plt.subplots(sharex=True, sharey=True)\n",
    "plot_potential_error.plot(r_sorted,potential_error, label = 'Relative Error')\n",
    "plot_potential_error.plot(r_sorted,potential_proportionality, label = 'Proportionality')\n",
    "plt.title('Error in the Potential')\n",
    "plot_annotations(plot_potential_error)\n",
    "plot_format(plot_potential_error,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the value of the potential along a specific axis. Useful when dealing with a non-radially symmetric distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(x_coords, potential, marker = '.', s = 0.5, c = y_coords/y_coords.max(), cmap = 'jet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, the acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Defining analytic functions to check if the result is correct\n",
    "#2nd argument of Heaviside is its value where the Heaviside goes from 0 to 1\n",
    "acceleration_sphere_analytic = (np.heaviside(r_sorted - radius_tot, 0.5)*sqrt(G*mgb*a0)*1/r_sorted+\n",
    "(np.heaviside(radius_tot-r_sorted, 0.5))*4/3*sqrt(pi/3*a0*G*mgb/volume_out)*3/2*np.sqrt(r_sorted))\n",
    "\n",
    "acceleration_dirac_analytic = sqrt(G*mgb*a0)*1/r_sorted\n",
    "\n",
    "fig, acceleration1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "acceleration1.set_ylabel('FEM', color=color)\n",
    "\n",
    "acceleration1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#Plotting radial FEM solution and analytic solution on the same plot\n",
    "acceleration1.plot(r_sorted, acceleration_magnitude_sorted, label = 'FEM', color = color)\n",
    "\n",
    "#UNCOMMENT TO HAVE SEPARATE Y AXES\n",
    "# acceleration2 = acceleration1.twinx()\n",
    "color = 'tab:blue'\n",
    "# acceleration2.set_ylabel('Analytic', color=color)\n",
    "\n",
    "plt.plot(r_sorted, acceleration_sphere_analytic, label = 'Analytic', linestyle = '--', color = color)\n",
    "plt.title('Gravitational Acceleration')\n",
    "\n",
    "plot_annotations(acceleration1)\n",
    "\n",
    "#Formatting plot using the function I made\n",
    "plot_format(acceleration1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the error in the acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for spherically symmetric mass distributions we have the anlytic solution, so we can compute\n",
    "#the error\n",
    "acceleration_error = np.abs((acceleration_magnitude_sorted - acceleration_dirac_analytic)/acceleration_magnitude_sorted)\n",
    "\n",
    "#plotting the error against the radius\n",
    "fig, acceleration_error_plot = plt.subplots()\n",
    "acceleration_error_plot.plot(r_sorted,acceleration_error, label = 'Relative Error')\n",
    "plt.title('Error in the Acceleration')\n",
    "plot_annotations(acceleration_error_plot)\n",
    "plot_format(acceleration_error_plot,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the actual mass distribution that we input in the PDE, correpsonding to the baryonic matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, source_radial_plot = plt.subplots()\n",
    "\n",
    "#Scaling the mass distribution by 4*pi*G to get rho itself\n",
    "source_radial_plot.plot(r_sorted, 1/(4*pi*G)*source_sorted)\n",
    "\n",
    "plt.title('Source')\n",
    "plot_annotations(source_radial_plot)\n",
    "plot_format(source_radial_plot,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the laplacian of the solution, that for MOND corresponds to the total matter distribution, baryons+dark matter. For Newton it should correspond to the mass distribution that we input in the PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, apparent_mass_plot = plt.subplots()\n",
    "\n",
    "plot_together = True\n",
    "\n",
    "#Scaling the mass distribution by 4*pi*G to get rho itself\n",
    "apparent_mass_plot.plot(r_sorted, apparent_mass_distribution_sorted, label = 'Apparent Mass Distribution')\n",
    "\n",
    "if plot_together == True:\n",
    "    \n",
    "    apparent_mass_plot.plot(r_sorted, source_sorted, label = 'Baryonic Mass Distribution', linestyle='--')\n",
    "\n",
    "plt.title('Apparent Mass Distribution')\n",
    "plot_annotations(apparent_mass_plot)\n",
    "plot_format(apparent_mass_plot,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "\n",
    "    fig, apparent_mass_total_plot = plt.subplots()\n",
    "\n",
    "    #Scaling the mass distribution by 4*pi*G to get rho itself\n",
    "    apparent_mass_total_plot.plot(r_total_sorted, apparent_mass_total_sorted, label = 'Apparent Mass Distribution')\n",
    "\n",
    "    apparent_mass_total_plot.plot(r_total_sorted, source_total_sorted, label = 'Baryonic Mass Distribution', linestyle='--')\n",
    "\n",
    "    plt.title('Apparent Mass Distribution')\n",
    "    plot_annotations(apparent_mass_total_plot)\n",
    "    plot_format(apparent_mass_total_plot,1,1)\n",
    "    \n",
    "    #Saving the figure in the Figure folder, removed padding arounf with bbox_inches and \n",
    "    plt.savefig(f'Figures/total_mass_distribution.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the difference between the apparent mass distribution obtained as the Laplacian of the solution, and the baryonic mass distribution which is the RHS of the PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The difference between apparent mass and baryonic mass is the dark matter distribution\n",
    "dark_matter_density_sorted = (apparent_mass_distribution_sorted-source_sorted)\n",
    "\n",
    "fig, dark_matter_density_plot = plt.subplots()\n",
    "\n",
    "dark_matter_density_plot.plot(r_sorted, dark_matter_density_sorted)\n",
    "plt.title('Dark Matter Distribution')\n",
    "plot_annotations(dark_matter_density_plot)\n",
    "plot_format(dark_matter_density_plot,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The ratio between apparent mass and baryonic mass is the dark matter distribution\n",
    "dark_matter_ratio_sorted = (apparent_mass_distribution_sorted/source_sorted)\n",
    "\n",
    "fig, dark_matter_ratio_plot = plt.subplots()\n",
    "\n",
    "dark_matter_ratio_plot.plot(r_sorted, dark_matter_ratio_sorted)\n",
    "plt.title('Dark Matter Ratio')\n",
    "plot_annotations(dark_matter_ratio_plot)\n",
    "plot_format(dark_matter_ratio_plot,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram to look at the radial vertex distribution\n",
    "### First, defining the function to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the function to the generated mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_dist_hist(r_sorted, mesh, False, 10)\n",
    "\n",
    "radial_plots_end = time.time()\n",
    "radial_plots_time = run_time(radial_plots_start - radial_plots_end, 'Radial Plots')\n",
    "# section_times.append(radial_plots_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots_3D_start = time.time()\n",
    "\n",
    "if plot_3D_graphs: \n",
    "\n",
    "    #Have to first declare a figure and use its name as an input to the function\n",
    "    #This way the plot can be plotted alongside other plots on the same grid\n",
    "    whole_mesh = plt.figure()\n",
    "\n",
    "    #Plotting the points \n",
    "    plot_mesh(mesh, Point(center_of_mass), degree_PDE, whole_mesh, 1, acceleration_magnitude_sorted, show_mesh=True, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For non spherically symmetric meshes, and for visual clarity, taking a slice of the mesh and plotting it in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if plot_3D_graphs: \n",
    "    \n",
    "    mesh_plane = plt.figure()\n",
    "    plot_mesh_slice(20, mesh_plane, mesh, Point(center_of_mass), degree_PDE, random_coordinates, height = center_of_mass_z, portion = True, cross_section=True, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_3D_graphs:\n",
    "\n",
    "    trisurf_potential = plt.figure()\n",
    "    trisurf_function_slice(trisurf_potential, potential, 20, center_of_mass_z, mesh, Point(center_of_mass), degree_PDE)\n",
    "    plt.title('Potential in xy-plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (plot_3D_graphs and acceleration_needed):  \n",
    "    \n",
    "    trisurf_acceleration = plt.figure()\n",
    "    trisurf_function_slice(trisurf_acceleration, acceleration_magnitude, 20, center_of_mass_z, mesh, Point(center_of_mass), degree_PDE)\n",
    "    plt.title('Acceleration in xy-plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trisurf_source = plt.figure()\n",
    "# trisurf_function_slice(trisurf_source, source, 20, center_of_mass_z, high_low = 'high')\n",
    "# plt.title('Source in xy-plane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting contour lines of the potential, so we can do that for different values of z and see the whole domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tricontour_potential = plt.figure()\n",
    "tricontour_function_slice(tricontour_potential, potential, Point(center_of_mass), 20, 20, center_of_mass_z, mesh, degree_PDE)\n",
    "plt.title('Potential in xy-plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if acceleration_needed:\n",
    "\n",
    "    tricontour_acceleration = plt.figure()\n",
    "    tricontour_function_slice(tricontour_acceleration, acceleration_magnitude, Point(center_of_mass), 20, 20, center_of_mass_z, mesh, degree_PDE)\n",
    "    plt.title('Acceleration in xy-plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricontour_source = plt.figure()\n",
    "tricontour_function_slice(tricontour_source, source, Point(center_of_mass), 20, 20, center_of_mass_z, mesh, degree_PDE)\n",
    "plt.title('Source in xy-plane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to plot slices and view them in 3D\n",
    "### The 2 cells below this on contain the call to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: Right now, using a predefined amount of contours for each level, but this means\n",
    "#that the colors ar enot consistent between different levels! So want to change it that instead\n",
    "#of a numebr of contours, we have contours at specific values! That way the plot makes sense,\n",
    "#and it will also look like a sphere since the max potential will be at the boundary, which \n",
    "#becomes smaller for each level.\n",
    "\n",
    "# potential_slices = plt.figure()\n",
    "\n",
    "# contour_3D_slices(potential_slices, potential, 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceleration_slices = plt.figure()\n",
    "\n",
    "# contour_3D_slices(acceleration_slices, acceleration_magnitude, 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_slices = plt.figure()\n",
    "\n",
    "# contour_3D_slices(source_slices, source, 100, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at a quiver plot of the acceleration (useful when having multiple masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "\n",
    "quivers = figure.add_subplot(111, projection='3d')\n",
    "\n",
    "quivers.quiver(x_coords, y_coords, z_coords, acceleration_x, acceleration_y, acceleration_z, length=domain_size, normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots_3D_end = time.time()\n",
    "plots_3D_time = run_time(plots_3D_end - plots_3D_start, '3D Plots')\n",
    "section_times.append(plots_3D_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the times taken by each section to profile the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pie_name = [section.name for section in section_times]\n",
    "pie_time = np.zeros((len(section_times),1))\n",
    "\n",
    "#can't use a list comprehension as for pie_name to make a numpy array, cause it makes\n",
    "#a list instead! and for numbers it's always best to work with numpy \n",
    "for i, section in enumerate(section_times):\n",
    "    pie_time[i] = section.time\n",
    "    \n",
    "#percentage of time taken, to display on the pie chart\n",
    "pie_time_percent = [pie_time/(pie_time.sum()*100)]\n",
    "\n",
    "#plotting the pie chart\n",
    "plt.pie(pie_time, labels = pie_name)\n",
    "# plt.legend()\n",
    "plt.title('Computation Times per Section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall time taken for process {rank}: {time.time() - starting_time} s \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to close all figures so it doesnt take up all the memory\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other instance of main solver to either compare solutions or explore parameter space etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_solutions(PDE_List, max_value, variable_name, samples, variable_title, title_units):\n",
    "    '''This function accepts a list of BVP object, containing weak form, source, initial guess\n",
    "    and name for a given BVP. Max value is the maximum value of a variable that is to be analysed,\n",
    "    and looped over, variable_name is its name, samples is the amount of values to be taken between\n",
    "    zero and max_value when looping over the variable, and variable_title is used to name subplots'''\n",
    "    \n",
    "    print(f'{variable_name} = {eval(variable_name)}')\n",
    "    \n",
    "    #Defining the amount of subplots needed from the samples input. We want samples/2 columns, and 2 rows\n",
    "    #For it to work for odd numbers, we add the remainder of 2 so it's always divisible, then convert\n",
    "    #to integer\n",
    "    subplot_layout = (int((samples+samples%2)/2),2)\n",
    "    \n",
    "    #If we have only one sample, we make a plot with no subplots.\n",
    "    if samples == 1:\n",
    "    \n",
    "        fig, potential_compare = (plt.subplots(sharex=True,sharey=True))\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Making a figure before we loop so all plots go in the same one. Sharing x,y axes per column/row\n",
    "        #and getting rid of space inbetween graphs\n",
    "        fig, potential_compare = (plt.subplots(subplot_layout[0],subplot_layout[1],sharex=True,sharey=True))\n",
    "    #                                           gridspec_kw={'hspace': 0, 'wspace': 0}))\n",
    "    \n",
    "    #Defining an empty list to hold the solutions of each PDE, with #elements equal to samples\n",
    "    u_list = [0 for sample in range(len(PDE_List))]\n",
    "\n",
    "    #Defining a list of the potentials from the solution from each weak form\n",
    "    potential_list = u_list\n",
    "\n",
    "    #List for the sorted potentials\n",
    "    potential_sorted_list = u_list\n",
    "    \n",
    "    #We loop over the chosen variable, going from its max_value/sample number to its max value\n",
    "    for j, variable_value in enumerate(np.linspace(1,samples,samples)*max_value/samples):\n",
    "        \n",
    "        #Assigning the value of variable_value to the variable (e.g. standard deviation, #source etc.)\n",
    "        #We need to evaluate it from here through te variable name in order for it to correctly pass it\n",
    "        #to the c++ code in the solver. {variable name}. Must use exec, not eval for assignment\n",
    "        exec(f'{variable_name} = {variable_value}')\n",
    "        \n",
    "        print(f'{variable_name} = {eval(variable_name)}')\n",
    "        \n",
    "        #If there are two or fewer subplots, the subplot index must actually be a number, not a tuple.\n",
    "        #Either 0 or 1, so same as j.\n",
    "        if samples <= 2:\n",
    "\n",
    "            subplot_index = j\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            #Converting the index i to binary, then using it to indicate the subplots to use [0,0], [0,1]\n",
    "            # and so on\n",
    "            subplot_index = ((int(j/2), j%2))\n",
    "        \n",
    "        #Looping over weak forms for the same initial guess and source\n",
    "        for i, PDE in enumerate(PDE_List):\n",
    "            \n",
    "            #Putting the solutions in the u_list list\n",
    "            u_list[i], f = solve_PDE(PDE)\n",
    "\n",
    "            #Getting the potentials and sorted potentials for each solution\n",
    "            potential_list[i] = u_list[i].compute_vertex_values()\n",
    "            potential_sorted_list[i] = potential_list[i][sorting_index]\n",
    "            \n",
    "            #If we have the Newtonian potential, we add the difference at the boundary so they are on the\n",
    "            #same part of the plot. Doesnt matter that we add a constant to the potential anyway\n",
    "            if 'Newton' in PDE.name:\n",
    "                \n",
    "                potential_sorted_list[i] = (potential_sorted_list[i] + sqrt(G*mgb*a0)*ln(domain_size))\n",
    "            \n",
    "            #If we have only one sample, we plot on the plot itself, not a subplot\n",
    "            if samples == 1:\n",
    "                \n",
    "                #Plot all the potentials on the same graph\n",
    "                potential_compare.plot(r_sorted, potential_sorted_list[i], label = PDE.name)\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                #Plot all the potentials on the same graph\n",
    "                potential_compare[subplot_index].plot(r_sorted, potential_sorted_list[i], label = PDE.name)\n",
    "        \n",
    "        #Making a string for the title of each plot. The :.2E is to format the number in the title to be\n",
    "        #in powers of 10.\n",
    "        plot_title = f'${variable_title} = {(int(variable_value)):.1E} \\: {title_units}$'\n",
    "        \n",
    "        #If we have only 1 sample, we need to call the function for the plot, rather than subplot\n",
    "        if samples == 1:\n",
    "        \n",
    "            #Giving a title to each subplot\n",
    "            potential_compare.set_title(plot_title)\n",
    "\n",
    "            #Formatting each subplot after it's been filled with all the curves\n",
    "            plot_format(potential_compare,1,1)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            potential_compare[subplot_index].set_title(plot_title)\n",
    "\n",
    "            plot_format(potential_compare[subplot_index],1,1)\n",
    "        \n",
    "    return u_list, potential_sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we compare the three interpolation functions (deep, simple, standard) for some different mass distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lists of same source, different weak form.\n",
    "BVP_dirac_list = [newton_dirac, mond_deep_dirac, mond_simple_dirac, mond_standard_dirac]\n",
    "BVP_gauss_list = [newton_gauss, mond_deep_gauss, mond_simple_gauss, mond_standard_gauss]\n",
    "BVP_continuous_list = [newton_continuous, mond_deep_continuous, mond_simple_continuous, mond_standard_continuous]\n",
    "\n",
    "#list of same weak form, different source.\n",
    "BVP_deep_list = [mond_deep_dirac, mond_deep_gauss, mond_deep_continuous]\n",
    "BVP_simple_list = [mond_simple_dirac, mond_simple_gauss, mond_simple_continuous]\n",
    "BVP_standard_list = [mond_standard_dirac, mond_standard_gauss, mond_standard_continuous]\n",
    "BVP_newton_list = [newton_dirac, newton_gauss, newton_continuous]\n",
    "\n",
    "if make_comparison:\n",
    "\n",
    "    #Running the compare function\n",
    "    discrete_list = compare_solutions(BVP_dirac_list, stand_dev, 'stand_dev', 1, '\\sigma = ', 'Mpc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
