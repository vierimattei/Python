{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining all the constants required (copying them from Matlab and adjusting syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dolfin import *\n",
    "\n",
    "#pandas is needed to import the cluster database\n",
    "import pandas as pd\n",
    "\n",
    "#Importing MPI for parallel computing\n",
    "# from mpi4py import MPI\n",
    "\n",
    "# #Importing the PETSc module for parallel use\n",
    "# from petsc4py import PETSc\n",
    "\n",
    "#importing mshr for all mesh functions\n",
    "import mshr as mshr\n",
    "\n",
    "# Use SymPy to compute f from the manufactured solution u\n",
    "import sympy as sym\n",
    "\n",
    "#Option to avoid printing redundant information from each core when running the code in parallel from\n",
    "#a python (.py) script obtained from the jupyter notebook.\n",
    "# parallel_run = True\n",
    "\n",
    "#MPI communicator\n",
    "comm = MPI.comm_world\n",
    "\n",
    "#Rank of each process (its ID essentially)\n",
    "rank = MPI.rank(comm)\n",
    "\n",
    "#Total number of processes\n",
    "number_processes = MPI.size(comm)\n",
    "\n",
    "print(f'This is process {rank} out of {number_processes-1}')\n",
    "\n",
    "if number_processes <2:\n",
    "\n",
    "    #Increasing the width of the notebook (visual difference only)\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    \n",
    "    #have to define where to put plots BEFORE importing matplotlib\n",
    "    %matplotlib notebook\n",
    "\n",
    "#Importing matplotlib to plot the results\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Importing numpy to work with arrays\n",
    "import numpy as np\n",
    "\n",
    "#Importing tempfile to save numpy arrays from the main script so we can get them back and plot them\n",
    "#interatively rather than saving a pdf or png!\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#Importing time to compute how long each segment takes\n",
    "import time\n",
    "\n",
    "#importing regex to change every instance of radius_tot so we change the ones in the C++ code\n",
    "#at the same time too\n",
    "import re\n",
    "\n",
    "#varname gives the name of the variable as a string\n",
    "from varname import varname\n",
    "\n",
    "#Needed to use the 3D scatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#Importing the decimal package to be able to specify arbitrary accuracy, needed e.g. when\n",
    "#calculating the jacobian for the lensing\n",
    "from decimal import *\n",
    "\n",
    "#Importing all quantities, constants etc used in the calculations\n",
    "from MONDquantities import *\n",
    "\n",
    "#Importing all classes I created\n",
    "from MONDclasses import *\n",
    "\n",
    "#Importing the functions I made from the MONDfunctions file\n",
    "from MONDfunctions import *\n",
    "\n",
    "#Importing all expressions for weak forms, initial guesses/BCs and sources\n",
    "from MONDexpressions import *\n",
    "\n",
    "#Needed if want to use the adapt function for mesh refinement, see:\n",
    "#https://fenicsproject.org/qa/6719/using-adapt-on-a-meshfunction-looking-for-a-working-example/\n",
    "#If using 'plaza' instead of 'plaza_with_parent_facets', it's faster by about 30%! Also, I get the\n",
    "#'*** Warning: Cannot calculate parent facets if redistributing cells'. So for MPI no need to use\n",
    "#with parent facets!\n",
    "parameters[\"refinement_algorithm\"] = \"plaza\"\n",
    "\n",
    "#Setting compiler parameters.\n",
    "#Optimisation\n",
    "parameters[\"form_compiler\"][\"optimize\"]     = True\n",
    "parameters[\"form_compiler\"][\"cpp_optimize\"] = True\n",
    "\n",
    "#Nonzero initial guess for the Krylov solver. Doesnt seem to make a difference for nonlinear problems\n",
    "# parameters['krylov_solver']['nonzero_initial_guess'] = True\n",
    "\n",
    "#Ghost mode for when using MPI. Each process gets ghost vertices for the part of the domain it does not\n",
    "#own. Have to set to 'none' instead or I get Error 'Unable to create BoundaryMesh with ghost cells.'\n",
    "parameters['ghost_mode'] = 'none'\n",
    "\n",
    "#Start of overall for loop over the parameters of the database\n",
    "\n",
    "#I deleted the original table by mistake, but luckily I had the dataframe open in adifferent notebook\n",
    "#so I saved it as a pickle. Now all I need to do to import it is read the pickle!\n",
    "df = pd.read_pickle('cluster_data_pickle.pkl')\n",
    "\n",
    "#Putting each column in its own list to loop over. Scaling rho_0 by 10^22, and rc by kp\n",
    "cluster_name = df.loc[:, 'name']\n",
    "cluster_rc = df.loc[:, 'r_c_frame']*kp\n",
    "cluster_rho0 = df.loc[:, 'rho_0_frame']*10**(-22)\n",
    "cluster_beta = df.loc[:, 'beta_frame']\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    #Setting the parameters for the beta distribution that we want to compute with\n",
    "    #The domain size is the Abell radius for all the clusters \n",
    "    domain_size = radius_abell\n",
    "    \n",
    "    #All the other quantities come from the cluster database for the respective index\n",
    "    r_c = cluster_rc[i]\n",
    "    rho_0 = cluster_rho0[i]\n",
    "    beta = cluster_beta[i]\n",
    "    \n",
    "    #Making a dummy variable going from 0 to the domain_size to get the total mass by integrating the beta\n",
    "    #gas density over the whole domain\n",
    "    r_integration = np.linspace(0,domain_size,10000)\n",
    "\n",
    "    mgb = (np.trapz(rho_0/(1+(r_integration/r_c)**2)**(3*beta/2)*\n",
    "                          4*pi*r_integration**2, x = r_integration))\n",
    "    \n",
    "    ## starting time of whole PDE solver\n",
    "    starting_time = time.time()\n",
    "\n",
    "    #starting an empty list to contain all of the run_time objects to plot later\n",
    "    section_times = []\n",
    "\n",
    "    print('Starting mesh generation...\\n')\n",
    "    mesh_generation_start = time.time()\n",
    "\n",
    "    #Making mesh from function defined above\n",
    "    mesh = make_spherical_mesh(domain_size, mesh_resolution)\n",
    "\n",
    "    mesh_generation_end = time.time()\n",
    "    mesh_generation_time = run_time(mesh_generation_end - mesh_generation_start, 'Mesh Generation')\n",
    "    section_times.append(mesh_generation_time)\n",
    "    print('Mesh generated in {} s \\n'.format(mesh_generation_time.time))\n",
    "\n",
    "    #Setting the MPI communicator for the mesh (doesnt seem to do anything right now)\n",
    "    # mesh.mpi_comm = comm\n",
    "\n",
    "    print(f'The mesh of process {rank} has {mesh.num_cells()} cells')\n",
    "\n",
    "    ## Defining coordinates for some test mass distributions\n",
    "\n",
    "    #For all the points to be within a given radius, each coordinate must be smaller than\n",
    "    #radius_population/sqrt(3)\n",
    "    random_max_distance = radius_population/sqrt(3)\n",
    "\n",
    "    #Setting a given seed so we can always have the same random numbers for now\n",
    "    np.random.seed(1)\n",
    "\n",
    "    #We want a mean of 0 so center of mass is in center, and the same standard deviation as the gaussian\n",
    "    #pulse. This means we sample from the same distribution as the smooth one, and have the same mean.\n",
    "    #This is exactly what we want to compare coarse and smooth distributions\n",
    "    mu, sigma = 0, stand_dev\n",
    "    random_coordinates_x = np.random.normal(mu, sigma, source_number)\n",
    "    random_coordinates_y = np.random.normal(mu, sigma, source_number)\n",
    "\n",
    "    #If we want all source to be in the same plane, we set the z axis to be 0 for all of them. Otherwise,\n",
    "    #random as above\n",
    "    if coplanar_sources == True:\n",
    "\n",
    "        random_coordinates_z = np.zeros((source_number, 1)).ravel()\n",
    "\n",
    "    else:\n",
    "\n",
    "        random_coordinates_z = np.random.normal(mu, sigma, source_number)\n",
    "\n",
    "    # # random_coordinates[0][0] = -domain_size/their_distance\n",
    "    if central_mass:\n",
    "\n",
    "        random_coordinates_x[0] = 0\n",
    "        random_coordinates_y[0] = 0\n",
    "        random_coordinates_z[0] = 0\n",
    "\n",
    "    #Overall array containing all coordinates. If over-writing the random position, this has to go after it,\n",
    "    #otherwise the c++ array for the source sets the wrong position!\n",
    "    random_coordinates = np.array((random_coordinates_x, random_coordinates_y, random_coordinates_z))\n",
    "    random_coordinates = np.transpose(random_coordinates)\n",
    "\n",
    "    #Obtaining the center of each source as a list of points\n",
    "    source_centers = [Point(random_coordinates_x[i], random_coordinates_y[i], random_coordinates_z[i]) for i in range(source_number)]\n",
    "\n",
    "    #For the current case in which all sources have the same mass, we simply divide by #sources\n",
    "    center_of_mass_x = random_coordinates[:,0].sum()/source_number\n",
    "    center_of_mass_y = random_coordinates[:,1].sum()/source_number\n",
    "    center_of_mass_z = random_coordinates[:,2].sum()/source_number\n",
    "\n",
    "    #Overall center of mass\n",
    "    center_of_mass = [center_of_mass_x, center_of_mass_y, center_of_mass_z]\n",
    "\n",
    "    # center_of_mass\n",
    "    print(f'Process {rank} about to refine')\n",
    "\n",
    "    print('Starting mesh refinement...\\n')\n",
    "    mesh_refine_start = time.time()\n",
    "    new_mesh = more_modified_refinement(mesh, source_centers, refine_times)\n",
    "    # new_mesh = local_refinement(mesh, source_centers, radius_refine, refine_times, technique = 'ring')\n",
    "    mesh_refine_end = time.time()\n",
    "    mesh_refine_time = run_time(mesh_refine_end - mesh_refine_start, 'Mesh Refinement')\n",
    "    section_times.append(mesh_refine_time)\n",
    "    print('Mesh refined in {} s \\n'.format(mesh_refine_time.time))\n",
    "\n",
    "    mesh = new_mesh\n",
    "\n",
    "    # Gathering all the data from the mesh AFTER having done the mesh refinement and defined the mesh for plotting\n",
    "\n",
    "    print('Rearranging mesh data\\n')\n",
    "    rearrange_start = time.time()\n",
    "\n",
    "    V, vertex_number, x_coords, y_coords, z_coords, r_coords, sorting_index, x_sorted, y_sorted, z_sorted, r_sorted = rearrange_mesh_data(mesh, center_of_mass, degree_PDE)\n",
    "\n",
    "    #To be able to gather the coordinate arrays with MPI, the coordinates need to be C_contiguous\n",
    "    x_coords, y_coords, z_coords, r_coords = [np.ascontiguousarray(coord_array) for coord_array in [x_coords, y_coords, z_coords, r_coords]]\n",
    "\n",
    "    rearrange_end = time.time()\n",
    "    rearrange_time = run_time(rearrange_end - rearrange_start, 'Mesh data rearrange')\n",
    "    section_times.append(rearrange_time)\n",
    "    print('Mesh data rearranged in {} s \\n'.format(rearrange_time.time))\n",
    "\n",
    "    # Defining a few BVP from combinations we use often. Naming scheme: 'weak form_source'\n",
    "\n",
    "    #BVPs for a discrete dirac mass distribution, for Newton and MOND with/out interpolations\n",
    "    newton_dirac = BVP(F_Newton, u_Newton, f_multiple_dirac, 'Newton, discrete dirac')\n",
    "    mond_deep_dirac = BVP(F_MOND_deep, u_displaced_cpp, f_multiple_dirac, 'Deep MOND, discrete dirac')\n",
    "    mond_simple_dirac = BVP(F_MOND_simple, u_displaced_cpp, f_multiple_dirac, 'Simple MOND, discrete dirac')\n",
    "    mond_standard_dirac = BVP(F_MOND_standard, u_displaced_cpp, f_multiple_dirac, 'Standard MOND, discrete dirac')\n",
    "\n",
    "    #BVPs for a discrete gauss mass distribution.\n",
    "    newton_gauss = BVP(F_Newton, u_Newton, f_multiple_gauss, 'Newton, discrete gauss')\n",
    "    mond_deep_gauss = BVP(F_MOND_deep, u_displaced_cpp, f_multiple_gauss, 'Deep MOND, discrete gauss')\n",
    "    mond_simple_gauss = BVP(F_MOND_simple, u_displaced_cpp, f_multiple_gauss, 'Simple MOND, discrete gauss')\n",
    "    mond_standard_gauss = BVP(F_MOND_standard, u_displaced_cpp, f_multiple_gauss, 'Standard MOND, discrete gauss')\n",
    "\n",
    "    #BVPs for a continuous distribution, for Newton and MOND with/out interpolations\n",
    "    newton_continuous = BVP(F_Newton, u_Newton, f_exponent_test, 'Newton, continuous gauss')\n",
    "    mond_deep_continuous = BVP(F_MOND_deep, u_displaced_cpp, f_exponent_test, 'Deep MOND, continuous gauss')\n",
    "    mond_simple_continuous = BVP(F_MOND_simple, u_displaced_cpp, f_exponent_test, 'Simple MOND, continuous gauss')\n",
    "    mond_standard_continuous = BVP(F_MOND_standard, u_displaced_cpp, f_exponent_test, 'Standard MOND, continuous gauss')\n",
    "\n",
    "    #BVPs for a three parameter beta distribution\n",
    "    newton_beta = BVP(F_Newton, u_Newton, f_gas_three_beta, 'Newton, beta')\n",
    "    mond_deep_beta = BVP(F_MOND_deep, u_sphere_cpp, f_gas_three_beta, 'Deep MOND, beta')\n",
    "    mond_simple_beta = BVP(F_MOND_simple, u_sphere_cpp, f_gas_three_beta, 'Simple MOND, beta')\n",
    "    mond_standard_beta = BVP(F_MOND_standard, u_sphere_cpp, f_gas_three_beta, 'Standard MOND, beta')\n",
    "    \n",
    "    # Trying an alternative method for assigning values inside the c++ expressions by using exec, to avoid the limit on eval!\n",
    "\n",
    "    #Defining a function for the boundary. Since we only have one BC for the whole boundary, we\n",
    "    #can make a simple function that returns true for each value on the boundary\n",
    "    #the on_boundary built-in function takes each point in domain and returns true if on boundary\n",
    "    def boundary(x, on_boundary):\n",
    "        return on_boundary\n",
    "\n",
    "    radius_tot=r_c\n",
    "\n",
    "    def solve_PDE(the_BVP):\n",
    "        '''Function takes in a BVP object, which defines the weak form, initial guess/BC and source for\n",
    "        a PDE, and computes its solution'''\n",
    "\n",
    "        ## starting time of PDE solver\n",
    "        solver_start = time.time()\n",
    "        print('Starting PDE Solver...\\n')\n",
    "\n",
    "        #Defining the source term here, cause the make_source_string function creates a string that \n",
    "        #evaluate the expression for a variable called 'source'\n",
    "        source = the_BVP.source\n",
    "\n",
    "        #Evaluating the source term obtained from the make_source_string function\n",
    "        f = eval(make_source_string(source_number))\n",
    "\n",
    "        #Declaring the expression for the initial guess\n",
    "        u = (Expression(the_BVP.initial_guess,\n",
    "        degree = degree_PDE, a0 = a0, ms = ms,mgb = mgb, G = G,  ly = ly, kp = kp, radius_tot = radius_tot,\n",
    "        volume_out = volume_out, center_of_mass_x = center_of_mass_x,\n",
    "        center_of_mass_y = center_of_mass_y, center_of_mass_z = center_of_mass_z,\n",
    "        source_number = source_number, source_mass = source_mass))\n",
    "\n",
    "        #Declaring the expression for the boundary condition with displaced CM (center of mass)\n",
    "        boundary_CM = u\n",
    "\n",
    "        #Declaring the boundary condition. It takes three arguments: function space, value of BC, \n",
    "        #section of the boundary (in our case the whole boundary).\n",
    "        bc = DirichletBC(V, boundary_CM, boundary)\n",
    "\n",
    "        #Defining the variational problem\n",
    "        #u is the solution. for linear problems, we'd have to define it as TrialFunction, but for \n",
    "        #non-linear we define it as Function directly\n",
    "        u = interpolate(u, V)\n",
    "\n",
    "        #defining the test function\n",
    "        v = TestFunction(V)\n",
    "\n",
    "        #defining the weak form to be solved\n",
    "        F = eval(the_BVP.weak_form)\n",
    "\n",
    "        #Computing the solution for normal deep MOND\n",
    "        (solve(F == 0, u, bc, solver_parameters={\"newton_solver\":{\"relative_tolerance\":1e-6},\n",
    "                                                 \"newton_solver\":{\"maximum_iterations\":200}}))\n",
    "\n",
    "        solver_end = time.time()\n",
    "        solver_time = run_time(solver_end - solver_start, 'PDE Solver')\n",
    "        section_times.append(solver_time)\n",
    "\n",
    "        print('PDE solved in {}\\n'.format(solver_time.time))\n",
    "\n",
    "        return u, f\n",
    "\n",
    "    #Waiting for each process to have completed before moving on to solve the PDE\n",
    "    # MPI.barrier(comm)\n",
    "\n",
    "    #Defined the quantity BVP_to_solve in the MONDquantities file as a string, so to use it we need to \n",
    "    #evaluate it with eval.\n",
    "    u, f = solve_PDE(eval(BVP_to_solve))\n",
    "\n",
    "    data_collection_start = time.time()\n",
    "    print('Collecting data from PDE...\\n')\n",
    "\n",
    "    if plotting_option == True:\n",
    "        mesh = mesh_for_plots\n",
    "        V_plot, vertex_number, x_coords, y_coords, z_coords, r_coords, sorting_index, x_sorted, y_sorted, z_sorted, r_sorted = rearrange_mesh_data(mesh, center_of_mass)\n",
    "        u_plot = interpolate(u, V_plot)\n",
    "        f_plot = interpolate(f, V_plot)\n",
    "        u = u_plot\n",
    "        #Calling the rearrange_mesh_data function to get coordinates and order them based on the \n",
    "        #distance from the center of mass\n",
    "\n",
    "    #The value of the function at each vertex of the mesh is stored in a np array. Its order\n",
    "    #corresponds to the otder of the mesh.coordinates() values\n",
    "    potential = u.compute_vertex_values()\n",
    "\n",
    "    #The value of the source at each vertex of the mesh\n",
    "    source = 1/(4*pi*G)*f.compute_vertex_values(mesh)\n",
    "\n",
    "    #Getting the degree from the scalar function space V from the PDE\n",
    "    degree = V.ufl_element().degree()\n",
    "\n",
    "    #Laplacian of the solution to get back the scaled mass distribution\n",
    "    lap = div(grad(u))\n",
    "\n",
    "    apparent_mass_project = project(lap, V)\n",
    "\n",
    "    #Projecting the acceleration onto a vector space is expensive, so don't do it unless needed\n",
    "    #If not needed, set acceleration to 0 everywhere\n",
    "    if acceleration_needed:\n",
    "\n",
    "        #To obtain the values for the acceleration, we need to define a new function space, since the \n",
    "        #gradient is a vector function is the function space for the PDE is a scalar function space\n",
    "        W = VectorFunctionSpace(mesh, 'P', degree)\n",
    "\n",
    "        #Projecting (similar to interpolating) the grad(u) field onto W, gives a function\n",
    "        acceleration_project = project(grad(u), W)\n",
    "\n",
    "        #The result of project is n*3,1 np.array, with 3 (x,y,z) values for each of the n vertices\n",
    "        acceleration = acceleration_project.compute_vertex_values()\n",
    "\n",
    "        #reshaping the array to split the x,y,z components into their own column each\n",
    "        acceleration = np.reshape(acceleration, (3, int(acceleration.shape[0]/3)))\n",
    "\n",
    "    else:\n",
    "\n",
    "        acceleration = np.zeros((3, len(potential)))\n",
    "\n",
    "\n",
    "    acceleration_x = acceleration[0]\n",
    "    acceleration_y = acceleration[1]\n",
    "    acceleration_z = acceleration[2]\n",
    "\n",
    "    #Finding the magnitude of the acceleration\n",
    "    acceleration_magnitude = np.linalg.norm(acceleration, axis=0)\n",
    "\n",
    "    #Sorting the potential, acceleration and source according to thr r of the vertex they pertain to\n",
    "    potential_sorted = potential[sorting_index]\n",
    "    acceleration_magnitude_sorted = acceleration_magnitude[sorting_index]\n",
    "    source_sorted = source[sorting_index]\n",
    "\n",
    "    data_collection_end = time.time()\n",
    "    data_collection_time = run_time(data_collection_end - data_collection_start, 'Data Collection')\n",
    "    section_times.append(data_collection_time)\n",
    "    print('Data collected in {} s\\n'.format(data_collection_time.time))\n",
    "\n",
    "    # Calculating the Laplacian of the potential to obtain the apparent dark matter distribution.\n",
    "\n",
    "    # #Projecting the divergence above onto the same scalar function space as the potential\n",
    "    # apparent_mass_project = project(apparent_mass_divergence, V)\n",
    "\n",
    "    integral = assemble(lap*dx)\n",
    "\n",
    "    #Gathering the values of the mass distribution \n",
    "    apparent_mass_distribution = 1/(4*pi*G)*apparent_mass_project.compute_vertex_values()\n",
    "\n",
    "    #Sorting the mass distribution values\n",
    "    apparent_mass_distribution_sorted = apparent_mass_distribution[sorting_index]\n",
    "\n",
    "    (integral/(4*pi*G))/mgb\n",
    "\n",
    "    # Gathering the potential and coordinate numpy array onto process 0 to have the full solution.\n",
    "\n",
    "    #First, we need to know how many vertices we have in total in the full mesh to preallocate the array\n",
    "    #for both the potential and the coordinates. We do this with the MPI reduce operation MPI_SUM\n",
    "    print(f'Process {rank}: potential has {len(potential)} elements.')\n",
    "\n",
    "    #We need the total #vertices as an int to define an array. Calling MPI.sum with communicator and\n",
    "    #value to be summed from each process\n",
    "    total_mesh_vertices = int(MPI.sum(comm, len(potential)))\n",
    "\n",
    "    if rank == 0:\n",
    "\n",
    "        print(f'Process {rank}: the overall potential has {total_mesh_vertices} elements.')\n",
    "\n",
    "    #Now we can gather all values of the potential and coordinates. First, we define arrays to hold the\n",
    "    #result, the size of the total potential on process 0\n",
    "\n",
    "    #Have to initialise the receving buffer for the potential to None on all processes or we get an error\n",
    "    potential_total = None\n",
    "    x_coords_total = None\n",
    "    y_coords_total = None\n",
    "    z_coords_total = None\n",
    "    r_coords_total = None\n",
    "    source_total = None\n",
    "    apparent_mass_total = None\n",
    "\n",
    "    if rank == 0:\n",
    "\n",
    "        #There is a problem with the receive buffer not being big enough. A simple fix for now is to \n",
    "        #multiply its size by 1.5, then we can remove all the trailing zeros\n",
    "        receiver_size = int(1.5*total_mesh_vertices)\n",
    "\n",
    "        potential_total = np.empty(receiver_size, dtype = type(potential[0]))\n",
    "        x_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "        y_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "        z_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "        r_coords_total = np.empty(receiver_size, dtype = type(x_coords[0]))\n",
    "        source_total = np.empty(receiver_size, dtype = type(potential[0]))\n",
    "        apparent_mass_total = np.empty(receiver_size, dtype = type(potential[0]))\n",
    "\n",
    "    #IMPORTANT: Have to use Gatherv, not Gather, or it won't work!\n",
    "    comm.Gatherv(potential, potential_total, root = 0)\n",
    "    comm.Gatherv(x_coords, x_coords_total, root = 0)\n",
    "    comm.Gatherv(y_coords, y_coords_total, root = 0)\n",
    "    comm.Gatherv(z_coords, z_coords_total, root = 0)\n",
    "    comm.Gatherv(r_coords, r_coords_total, root = 0)\n",
    "    comm.Gatherv(source, source_total, root = 0)\n",
    "    comm.Gatherv(apparent_mass_distribution, apparent_mass_total, root = 0)\n",
    "\n",
    "    #Now we want to sort as usual, now for the total potential and based on the overall r coordinates\n",
    "\n",
    "    if rank == 0:\n",
    "\n",
    "        #Storing the index to sort according to the total r\n",
    "        sorting_index_total = r_coords_total.argsort()\n",
    "\n",
    "        #Sorting all total quantities\n",
    "        r_total_sorted = r_coords_total[sorting_index_total]\n",
    "\n",
    "        x_total_sorted = x_coords_total[sorting_index_total]\n",
    "\n",
    "        y_total_sorted = y_coords_total[sorting_index_total]\n",
    "\n",
    "        z_total_sorted = z_coords_total[sorting_index_total]\n",
    "\n",
    "        potential_total_sorted = potential_total[sorting_index_total]\n",
    "\n",
    "        source_total_sorted = source_total[sorting_index_total]\n",
    "\n",
    "        apparent_mass_total_sorted = apparent_mass_total[sorting_index_total]\n",
    "\n",
    "        #Finding the zero elements in the sorted r array, and removing them. We do this by only keeping the\n",
    "        #Finding the indices for which r is larger than the smallest r on process 0, divided by 10**5 just to\n",
    "        #make sure. There should definitely not be any mesh points with distances smaller than that!\n",
    "        total_nonzero_indices = (r_total_sorted > r_sorted[0]/(10**5))\n",
    "\n",
    "        #Taking the non-padding components of radius, potential, source and mass distribution\n",
    "        x_total_sorted = x_total_sorted[total_nonzero_indices]\n",
    "        y_total_sorted = y_total_sorted[total_nonzero_indices]\n",
    "        z_total_sorted = z_total_sorted[total_nonzero_indices]\n",
    "        r_total_sorted = r_total_sorted[total_nonzero_indices]\n",
    "        potential_total_sorted = potential_total_sorted[total_nonzero_indices]\n",
    "        source_total_sorted = source_total_sorted[total_nonzero_indices]\n",
    "        apparent_mass_total_sorted = apparent_mass_total_sorted[total_nonzero_indices]\n",
    "\n",
    "        #The dark matter is the difference between the apparent and source masses\n",
    "        dark_mass_total_sorted = apparent_mass_total_sorted - source_total_sorted\n",
    "\n",
    "        #Saving all these numpy arrays so we can plot them again in Python, instead of just having a saved figure\n",
    "        #that is not interactive!\n",
    "\n",
    "        #Saving all the quantities to the respective files\n",
    "        np.save(f'database_results/{cluster_name[i]}/potential_{cluster_name[i]}.npy', potential_total_sorted)\n",
    "        np.save(f'database_results/{cluster_name[i]}/source_{cluster_name[i]}.npy', source_total_sorted)\n",
    "        np.save(f'database_results/{cluster_name[i]}/apparent_{cluster_name[i]}.npy', apparent_mass_total_sorted)\n",
    "        np.save(f'database_results/{cluster_name[i]}/dark_mass_{cluster_name[i]}.npy', dark_mass_total_sorted)\n",
    "        np.save(f'database_results/{cluster_name[i]}/x_sorted_{cluster_name[i]}.npy', x_total_sorted)\n",
    "        np.save(f'database_results/{cluster_name[i]}/y_sorted_{cluster_name[i]}.npy', y_total_sorted)\n",
    "        np.save(f'database_results/{cluster_name[i]}/z_sorted_{cluster_name[i]}.npy', z_total_sorted)\n",
    "        np.save(f'database_results/{cluster_name[i]}/r_sorted_{cluster_name[i]}.npy', r_total_sorted)\n",
    "\n",
    "    if rank == 0:\n",
    "\n",
    "        print(f'potential_total has length {len(potential_total)}')\n",
    "\n",
    "        potential_total_no_zeros = potential_total[np.nonzero(potential_total)]\n",
    "\n",
    "        print(f'potential_total_no_zeros has length {len(potential_total_no_zeros)}')\n",
    "\n",
    "\n",
    "        print(f'x_coords has length: {len(x_coords)}')\n",
    "        print(f'x_coords_total has length: {len(x_coords_total)}')\n",
    "        # x_coords_total\n",
    "        x_total_no_zeros = x_coords_total[np.nonzero(x_coords_total)]\n",
    "        print(f'x_total_no_zeros has length: {len(x_total_no_zeros)}')\n",
    "\n",
    "    # Other instance of main solver to either compare solutions or explore parameter space etc.\n",
    "\n",
    "    ## First, we compare the three interpolation functions (deep, simple, standard) for some different mass distributions.\n",
    "\n",
    "    #Lists of same source, different weak form.\n",
    "    BVP_dirac_list = [newton_dirac, mond_deep_dirac, mond_simple_dirac, mond_standard_dirac]\n",
    "    BVP_gauss_list = [newton_gauss, mond_deep_gauss, mond_simple_gauss, mond_standard_gauss]\n",
    "    BVP_continuous_list = [newton_continuous, mond_deep_continuous, mond_simple_continuous, mond_standard_continuous]\n",
    "\n",
    "    #list of same weak form, different source.\n",
    "    BVP_deep_list = [mond_deep_dirac, mond_deep_gauss, mond_deep_continuous]\n",
    "    BVP_simple_list = [mond_simple_dirac, mond_simple_gauss, mond_simple_continuous]\n",
    "    BVP_standard_list = [mond_standard_dirac, mond_standard_gauss, mond_standard_continuous]\n",
    "    BVP_newton_list = [newton_dirac, newton_gauss, newton_continuous]\n",
    "\n",
    "    if make_comparison:\n",
    "\n",
    "        #Running the compare function\n",
    "        discrete_list = compare_solutions(BVP_dirac_list, stand_dev, 'stand_dev', 1, '\\sigma = ', 'Mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential_total_sorted = np.load(f'database_results/{cluster_name[0]}/potential_{cluster_name[0]}.npy')\n",
    "# source_total_sorted = np.load(f'database_results/{cluster_name[0]}/source_{cluster_name[0]}.npy')\n",
    "# apparent_mass_total_sorted = np.load(f'database_results/{cluster_name[0]}/apparent_{cluster_name[0]}.npy')\n",
    "# dark_mass_total_sorted = np.load(f'database_results/{cluster_name[0]}/dark_mass_{cluster_name[0]}.npy')\n",
    "# x_total_sorted = np.load(f'database_results/{cluster_name[0]}/x_sorted_{cluster_name[0]}.npy')\n",
    "# y_total_sorted = np.load(f'database_results/{cluster_name[0]}/y_sorted_{cluster_name[0]}.npy')\n",
    "# z_total_sorted = np.load(f'database_results/{cluster_name[0]}/z_sorted_{cluster_name[0]}.npy')\n",
    "# r_total_sorted = np.load(f'database_results/{cluster_name[0]}/r_sorted_{cluster_name[0]}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Analytic potential for a MOND homogeneous sphere. Adding MOND potential at boundary for the offset\n",
    "# potential_sphere_MOND = (np.heaviside(r_total_sorted - radius_tot, 0.5)*sqrt(G*mgb*a0)*np.log(r_total_sorted) +\n",
    "# (np.heaviside(radius_tot - r_total_sorted, 0.5))*(4/3*sqrt(pi/3*a0*G*mgb/volume_out)*np.power(r_total_sorted,3/2)+\n",
    "# sqrt(G*mgb*a0)*ln(radius_tot) - 4/3*sqrt(pi/3*a0*G*mgb/volume_out)*radius_tot**(3/2)))\n",
    "\n",
    "# #Potential for a homogeneous sphere in Newton\n",
    "# potential_sphere_Newton = (np.heaviside(r_total_sorted - radius_tot, 0.5)*(-G*mgb/r_total_sorted) +\n",
    "# (np.heaviside(radius_tot - r_total_sorted, 0.5))*G*mgb/(2*radius_tot**3)*(r_total_sorted**2-\n",
    "# 3*radius_tot**2)+sqrt(G*mgb*a0)*ln(domain_size))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# plt.plot(r_total_sorted, potential_total_sorted)\n",
    "# plt.plot(r_total_sorted, potential_sphere_Newton, linestyle = '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "\n",
    "# plt.plot(r_total_sorted, source_total_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array to check all quantities are finite\n",
    "# is_finite = np.zeros((8,))\n",
    "\n",
    "# #Checking if all arrays have finite values only\n",
    "# for i, element in enumerate([potential_total_sorted, source_total_sorted, apparent_mass_total_sorted,\n",
    "#                 dark_mass_total_sorted, x_total_sorted, y_total_sorted, z_total_sorted,r_total_sorted]):\n",
    "    \n",
    "#     is_finite[i] = np.isfinite(element).any()\n",
    "    \n",
    "# print(f'The finite arrays: {is_finite}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
